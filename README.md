# Transformer Simplified
This is a series based activity, in each episode I'll explore/share a topic - that's simplified about transformer algorithm in the Deep Learning context.

# Inspiration
When I was interacting with LLMs, my first impression was that how they're so good at understanding the question you ask them. This triggered an enthusiasm in me to explore the root, and I found out that Transfomer architecture is the core behind it. Also, I found the "Attention is All you Need" paper a unique source to explain the whole architecture, but high level. So, I took courses, not actually Transfomer directly, but basics of Deep Learning -> CNNs -> RNNs (LSTM and GRU) -> then Transformer. Now, I'm excited to share the simplified of this journey with LLM enthusiast peers.

# Why do I do this?
Transformer is already available in libraries like `torch`, or even better, pre-trained ones on HuggingFace. But I do this, because I really want to know it from the root with every details - and I believe that's helpful even if you fine-tune a model, build agents, write custom tools and even prompting.

# Structure
Each episode will be inclosed inside a folder starting with Step_* followed by topic name covered.
Inside each step folder, there'll be a description.md file that contains breaks down the explanation of that topic. A pytorch-code.py file that will contain the python code to implement that step. Next step will include all the code from previous steps.

# Don't hesitate to reach out
Please feel free to contact me to discuss any topic or anything related further.
LinkedIn: [Amin Sherzad](https://www.linkedin.com/in/amin-sherzad/)
